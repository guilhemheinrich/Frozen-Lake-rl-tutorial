{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithmes d'apprentissage\n",
    "\n",
    "Au coeur de tous les algorithmes d'apprentissage présenté ici, il y a la matrice $Q_{s,a}$, qui est l'estimation relative (à l'algorithme) de chaque couple état-action. Plus la valeur du couple $(etat, action)$ est haute, plus elle est associé à un taux de réussite élevé, et pour un état donné (une ligne de la $Q_{s,a}$ ), la politique optimale associé choisira l'action ayant la valeur la plus haute.\n",
    "\n",
    "On va ici distinguer deux familles d'algorithmes:\n",
    "- d'une part la famille des algorithmes de Monte-Carlo, qui fonctionne dans un contexte épisodique, et qui a besoin d'un épisode complet mettre à jour la politique,\n",
    "- d'autre part la famille des algorithme de Temporal Difference (TD), qui mette à jour al politique à chaque étape de l'épisode"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methode(s) Monte-carlo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La méthode de **Monte-Carlo** (pour le *Reinforcement Learning*) est l'approche la plus \"naïve\" qu'on puisse avoir: pour évaluer la qualité d'un état, respectivement d'un couple état-action, on observe le \"taux de réussite\" de cet état, respectivement du couple état-action, après un épisode complet. \n",
    " A l'issue de suffisement d'épisodes, on a une estimation relative de chaque état, ou couple état-action, à partir de laquelle on peut déduire une politique \"optimale\".\n",
    "\n",
    "lien pertinent: https://towardsdatascience.com/monte-carlo-learning-b83f75233f92\n",
    "\n",
    "**Remarques**:\n",
    "- Comment faire lorsqu'on ne peut pas partir de chacun des états du système (comme c'est le cas dans *Frozen-Lake*)?\n",
    "- D'une certaine manière, on \"se fiche\" de comment l'agent parcours les états, seul l'observation des trajectoires comptent\n",
    "- Instinctivement, pour éviter de biaiser le parcours de l'agent, on peut utiliser une politique uniformément aléatoire (lors de la phase d'apprentissage)\n",
    "- Avec le constat précédent, on peut simplement paramétrer un algoritme de Monte Carlo avec comme seul paramètre le nombre d'itération"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from src.V2.Classes.Policy import Policy\n",
      "from src.V2.Classes.Agent import Agent\n",
      "\n",
      "import numpy as np\n",
      "import random\n",
      "\n",
      "# Generic Initialisation\n",
      "def MC(environment, epoch_number = 8000):\n",
      "    # Get the observation space & the action space\n",
      "    environment_space_length: int = environment.observation_space.n # type: ignore\n",
      "    action_space_length: int = environment.action_space.n # type: ignore\n",
      "    Q_sa = np.zeros((environment_space_length, action_space_length))\n",
      "    incremental_counter = np.zeros((environment_space_length, action_space_length))\n",
      "    random_policy = Policy(\n",
      "        lambda agent, state: \n",
      "            random.randint(0, action_space_length - 1)\n",
      "        ,\n",
      "        environment_space_length,\n",
      "        action_space_length\n",
      "    )\n",
      "    def update_MC(agent: Agent):\n",
      "        # All the (state, action) pair got updated with the last reward of the run\n",
      "        final_value = agent.trajectory.steps[-1].reward\n",
      "        for step in agent.trajectory.steps:\n",
      "            increment = incremental_counter[step.state, step.action]\n",
      "            old_value = Q_sa[step.state, step.action]\n",
      "            incremental_counter[step.state, step.action] += 1\n",
      "            Q_sa[step.state, step.action]= (increment * old_value + final_value) / incremental_counter[step.state, step.action]\n",
      "            \n",
      "    \n",
      "    for epoch in range(epoch_number):\n",
      "        stop = False\n",
      "        environment.reset()\n",
      "        learning_agent = Agent(random_policy, update_MC)\n",
      "        learning_agent.current_state_index = environment.reset()[0] # Initial state\n",
      "        action = learning_agent.pick_next()\n",
      "        while not stop:\n",
      "            '''\n",
      "            While Agent is not stopped:\n",
      "                Agent perform the action\n",
      "                Agent update its trajectory: action taken, new state, new reward\n",
      "                Agent pick an action from its new state according to its policy\n",
      "                Update the stop condition if Goal reached or terminated\n",
      "            '''\n",
      "            observation, reward, terminated, truncated, info = environment.step(action)\n",
      "            learning_agent.trajectory.append(learning_agent.current_state_index, action, float(reward))\n",
      "            learning_agent.current_state_index = observation\n",
      "            action = learning_agent.pick_next(observation)\n",
      "            stop = terminated or truncated\n",
      "        learning_agent.update()\n",
      "    return Q_sa\n"
     ]
    }
   ],
   "source": [
    "with open(\"Monte_Carlo.py\", 'r', encoding='utf-8') as the_file:\n",
    "    print(the_file.read())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Difference (TD)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contrairement aux algoritmes \"Monte Carlo\", les algorithmes TD mettent à jour leur politique à chaque étape de chaque épisode.\n",
    "\n",
    "**Remarques:**\n",
    "- J'ai trouvé beaucoup de contradiction dans les définitions de *on-policy* et *off-policy*, celles que j'ai utilisée sont la version de wikipedia (et découle de l'observation des méthodes de mise à jour)\n",
    "- pour ces algorithmes, on aura typiquement 4 paramètres: le nombre d'itération, le facteur d'apprentissage $\\alpha$, le facteur d'actualisation $\\gamma$ et $\\epsilon$ le facteur d'exporation de la politique $\\epsilon$*-greedy*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SARSA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'algorithme **SARSA** tire son nom de sa méthode d'apprentissage, qui signifie *State-Action-Reward-State-Action*: à chaque pas de l'algorithme, c'est à dire à chaque fois qu'un agent choisi une action, on va mettre à jour la matrice $Q_{s, a}$ en fonction de l'était de départ, l'action choisi par la politique, la récompense fournit par l'environnement, mais également l'action suivante à l'état suivant choisi par la politique en cours. L'amélioration de $Q_{s, a}$ peut donc s'écrire comme la fonction suivante: $update\\_SARSA(State_n, Action_n, Reward_n, State_{n+1}, Action_{n+1})$.\n",
    "\n",
    "Puisque que la fonction d'amélioration dépend de l'action $Action_{n+1}$, elle même déterminée par la politique en cours, on dit qu'il s'agit d'un algorithme *on-policy*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from typing import Callable\n",
      "import gymnasium as gym\n",
      "from nptyping import Float, NDArray, Shape\n",
      "import numpy as np\n",
      "\n",
      "from src.V2.Classes.Policy import Policy\n",
      "from src.V2.Classes.Agent import Agent\n",
      "from src.V2.Functions.epsilon_greedy_policy_factory import make_epsilon_greedy_policy\n",
      "\n",
      "def SARSA(environment, epsilon = 0.1, alpha = 0.1, gamma = 0.99, epoch_number = 8000):\n",
      "    # Get the observation space & the action space\n",
      "    environment_space_length: int = environment.observation_space.n # type: ignore\n",
      "    action_space_length: int = environment.action_space.n # type: ignore\n",
      "    Q_sa = np.zeros((environment_space_length, action_space_length))\n",
      "\n",
      "    epsilon_greedy_policy = Policy(\n",
      "        make_epsilon_greedy_policy(epsilon = epsilon, Q_sa = Q_sa),\n",
      "        environment_space_length,\n",
      "        action_space_length\n",
      "    )\n",
      "    def update_SARSA(agent: Agent, state_index: int, action_index: int, next_state: int, next_action: int,  reward:float = 0.0):\n",
      "        # Q[s, a] := Q[s, a] + Î±[r + Î³Q(s', a') - Q(s, a)]\n",
      "        Q_sa[state_index, action_index] = Q_sa[state_index, action_index] + alpha * (reward + gamma * Q_sa[next_state, next_action] - Q_sa[state_index, action_index])\n",
      "        # if Q_sa[state_index, action_index] != Q_sa[state_index, action_index] + alpha * (reward + gamma * Q_sa[next_state, next_action] - Q_sa[state_index, action_index]):\n",
      "        #     print(Q_sa[state_index, action_index] + alpha * (reward + gamma * Q_sa[next_state, next_action] - Q_sa[state_index, action_index]))\n",
      "\n",
      "\n",
      "    for epoch in range(epoch_number):\n",
      "        learning_agent = Agent(epsilon_greedy_policy, update_SARSA)\n",
      "        learning_agent.current_state_index = environment.reset()[0] # Initial state\n",
      "        stop = False\n",
      "        action = learning_agent.pick_next()\n",
      "        while not stop:\n",
      "            '''\n",
      "            While Agent is not stopped\n",
      "            Agent perform the action\n",
      "            Agent pick an action from its state and its policy\n",
      "            Agent update its trajectory: action taken, new state, new reward\n",
      "            Update the stop condition if Goal reached or terminated\n",
      "            '''\n",
      "            observation, reward, terminated, truncated, info = environment.step(action)\n",
      "            next_action_index = learning_agent.pick_next(observation)\n",
      "            learning_agent.trajectory.append(observation, next_action_index, float(reward))\n",
      "            learning_agent.update(\n",
      "                learning_agent.current_state_index,\n",
      "                action,\n",
      "                observation,\n",
      "                next_action_index,\n",
      "                reward)\n",
      "            learning_agent.current_state_index = observation\n",
      "            action = next_action_index\n",
      "            stop = terminated or truncated\n",
      "\n",
      "    # TODO Add some statistics\n",
      "    return Q_sa\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"SARSA.py\", 'r', encoding='utf-8') as the_file:\n",
    "    print(the_file.read())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'algorithme **Q_learning** ressemble beaucoup à l'algorithme **SARSA**: comme lui il va mettre à jour la matrice $Q_{s, a}$. L'amélioration de $Q_{s, a}$ peut s'écrire comme la fonction suivante: $update\\_Qlearning(State_n, Action_n, Reward_n, State_{n+1})$.\n",
    "\n",
    "Contrairement à la fonction d'almélioration de **SARSA**, on voit que celle de **Q_learning** ne dépend pas de l'action $Action_{n+1}$: elle ne dépend donc pas de la politique utilisé, et on parlera alors d'un algorithme *off-policy*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import gymnasium as gym\n",
      "import numpy as np\n",
      "\n",
      "from src.V2.Classes.Policy import Policy\n",
      "from src.V2.Classes.Agent import Agent\n",
      "from src.V2.Functions.epsilon_greedy_policy_factory import make_epsilon_greedy_policy\n",
      "\n",
      "def Q_learning(environment, epsilon = 0.1, alpha = 0.1, gamma = 0.99, epoch_number = 8000):\n",
      "    # Get the observation space & the action space\n",
      "    environment_space_length: int = environment.observation_space.n # type: ignore\n",
      "    action_space_length: int = environment.action_space.n # type: ignore\n",
      "    Q_sa = np.zeros((environment_space_length, action_space_length))\n",
      "\n",
      "    epsilon_greedy_policy = Policy(\n",
      "        make_epsilon_greedy_policy(epsilon = epsilon, Q_sa = Q_sa),\n",
      "        environment_space_length,\n",
      "        action_space_length\n",
      "    )\n",
      "    def update_Qlearning(agent: Agent, state_index, action_index, next_state, reward: float = 0):\n",
      "        # Q[s, a] := Q[s, a] + α[r + γ . argmax_a {Q(s', a')} - Q(s, a)]\n",
      "        best_next_action = np.argmax(Q_sa[next_state, ])\n",
      "        Q_sa[state_index, action_index] = Q_sa[state_index, action_index] + alpha * (reward + gamma * Q_sa[next_state, best_next_action] - Q_sa[state_index, action_index])\n",
      "\n",
      "\n",
      "\n",
      "    for epoch in range(epoch_number):\n",
      "        learning_agent = Agent(epsilon_greedy_policy, update_Qlearning)\n",
      "        learning_agent.current_state_index = environment.reset()[0] # Initial state\n",
      "        stop = False\n",
      "        action = learning_agent.pick_next()\n",
      "        while not stop:\n",
      "            '''\n",
      "            While Agent is not stopped\n",
      "            Agent perform the action\n",
      "            Agent pick an action from its state and its policy\n",
      "            Agent update its trajectory: action taken, new state, new reward\n",
      "            Update the stop condition if Goal reached or terminated\n",
      "            '''\n",
      "            observation, reward, terminated, truncated, info = environment.step(action)\n",
      "            next_action_index = learning_agent.pick_next(observation)\n",
      "            learning_agent.trajectory.append(observation, next_action_index, float(reward))\n",
      "            learning_agent.update(\n",
      "                learning_agent.current_state_index,\n",
      "                action,\n",
      "                observation,\n",
      "                reward)\n",
      "            learning_agent.current_state_index = observation\n",
      "            action = next_action_index\n",
      "            stop = terminated or truncated\n",
      "    return Q_sa\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"Q_learning.py\", 'r', encoding='utf-8') as the_file:\n",
    "    print(the_file.read())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparaisons, convergence et métrique"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les temps de calcul des algorithmes seront générallement \"long\", on voudrait alors savoir quand s'arrêter avec un résultat (*i.e* une politique optimale estimée) qui converge. Avant de se lancer dans une longue période d'apprentissage, on aimerait choisir judicieusement les paramètres de notre algorithme.\n",
    "\n",
    "**Remarques:**\n",
    "- La lecture de littérature sur le sujet peut donner une bonne idée de paramètres \"non-déconnants\", ou d'une plage de valeur pertinente\n",
    "- Ici, j'aurai une approche naïve, et j'essairai de choisir ses paramètres uniquement via l'exploitation des algorithmes précédents"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grille\n",
    "\n",
    "Pour essayer de me faire une idée du comportement de chaque algorithme en fonction de ses paramètre, l'approche naïve est de faire une grille sur l'espace des paramètres. Les valeurs possibles pour $\\alpha$, $\\gamma$ et $\\epsilon$ sont toutes dans l'intervalle $[0, 1]$, et les valeurs pour le nombre d'estimation sont les entiers positifs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reinforcment-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "633e522e98ac2168318462fbd68199107007513eae98ce5032c568ea8e2e0a77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
